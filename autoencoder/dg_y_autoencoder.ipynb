{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imagenes/rn3.png\" width=\"200\">\n",
    "<img src=\"http://www.identidadbuho.uson.mx/assets/letragrama-rgb-150.jpg\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Curso de Redes Neuronales](https://rn-unison.github.io)\n",
    "\n",
    "# Redes neuronales multicapa y el algoritmo de *b-prop*\n",
    "\n",
    "[**Julio Waissman Vilanova**](http://mat.uson.mx/~juliowaissman/), 27 de febrero de 2019.\n",
    "\n",
    "En esta libreta vamos a practicar con las diferentes variaciones del método de descenso de gradiente que se utilizan en el entrenamiento de redes neuronales profundas. Esta no es una libreta tutorial (por el momento, una segunda versión puede que si sea). Así, vamos a referenciar los algoritmos a tutoriales y artículos originales. Sebastian Ruder escribio [este tutorial que me parece muy bueno](http://ruder.io/optimizing-gradient-descent/index.html). Es claro, conciso y bien referenciado por si quieres mayor detalle. Nos basaremos en este tutorial para nuestra libreta.\n",
    "\n",
    "Igualmente, vamos a aprovechar la misma libreta para hacer y revisar como funcionan los *autocodificadores*. Los autocodificadores son muy importantes porque dan la intuición necesaria para introducir las redes convolucionales, y porque muestra el poder de compartir parámetros en diferentes partes de una arquitectura distribuida.\n",
    "\n",
    "Empecemos por inicializar los modulos que vamos a requerir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16,8)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Definiendo la red neuronal con arquitectura fija\n",
    "\n",
    "Como la definición de red neuronal, *f-prop* y *b-prop* ya fue tratados en otra libreta, vamos a inicializar una red neuronal sencilla, la cual tenga:\n",
    "\n",
    "1. Una etapa de autoencoder (usado para dos palabras)\n",
    "2. Una capa oculta con activación ReLU\n",
    "3. Una capa de salida con una neurona logística (problema de clasificación binaria)\n",
    "\n",
    "El número de salidas del autocodificador lo vamos a denotar como $n_a$, y el número de unidades ReLU de la capa oculta como $n_h$  \n",
    "\n",
    "A contonuación se agregan celdas de código para \n",
    "\n",
    "1. Inicialización de pesos\n",
    "2. Predicción (feed forward)\n",
    "3. El algoritmo de *b-prop* (calculo de $\\delta^{(1)} y $\\delta^{(2)}$)\n",
    "\n",
    "Si bien es bastante estandar algunas consideraciones se hicieron las cuales se resaltan más adelanta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializa_red(n_v, n_a, n_h):\n",
    "    \"\"\"\n",
    "    Inicializa la red neuronal\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    n_v : int con el número de palabras en el vocabulario\n",
    "    n_a : int con el número de características del autocodificador\n",
    "    n_h : int con el número de unidades ReLU en la capa oculta\n",
    "    \n",
    "    Devuelve\n",
    "    --------\n",
    "    W = [W_a, W_h, W_o] Lista con las matrices de pesos\n",
    "    B = [b_h, b_o] Lista con los sesgos\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(0) # Solo para efectos de reproducibilidad\n",
    "    \n",
    "    W_ac = np.random.randn(n_v, n_a)\n",
    "    W_h = np.random.randn(n_h, 2 * n_a)\n",
    "    W_o = np.random.randn(1, n_h)\n",
    "    \n",
    "    b_h = np.random.randn(n_h,1)\n",
    "    b_o = np.sqrt(n_h) * (2 * np.random.rand() - 1.0)\n",
    "    \n",
    "    return [W_ac, W_h, W_o], [b_h, b_o]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(A):   \n",
    "    \"\"\"\n",
    "    Calcula el valor de ReLU de una matriz de activaciones A\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.maximum(A, 0)\n",
    "\n",
    "def logistica(a):\n",
    "    \"\"\"\n",
    "    Calcula la funcion logística de a\n",
    "    \n",
    "    \"\"\"\n",
    "    return 1. / (1. + np.exp(-a))\n",
    "\n",
    "\n",
    "def feedforward(X, vocab, W, b):\n",
    "    \"\"\"\n",
    "    Calcula las activaciones de las unidades de la red neuronal\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    X: un ndarra [-1, 2], dtype='str', con dos palabras por ejemplo\n",
    "    vocab: Una lista con las palabras ordenadas del vocabulario a utilizar\n",
    "    W: Lista con las matrices de pesos (ver inicializa_red para mas info)\n",
    "    b: Lista con los vectores de sesgos (ver inicializa_red para mas info)\n",
    "    \"\"\"\n",
    "    W_a, W_h, W_o = W\n",
    "    b_h, b_o = b\n",
    "\n",
    "    one_hot_1 = [vocab.index(x_i) if x_i in vocab else -1 for x_i in X[:,0]]\n",
    "    one_hot_2 = [vocab.index(x_i) if x_i in vocab else -1 for x_i in X[:,1]]\n",
    "\n",
    "    activacion_z = np.array([one_hot_1, one_hot_2])\n",
    "    \n",
    "    activacion_a = np.r_[W_a[one_hot_1, :].T, \n",
    "                         W_a[one_hot_2, :].T]\n",
    "\n",
    "    activacion_h = relu(W_h @ activacion_a + b_h)\n",
    "    activacion_o = logistica(W_o @ activacion_h + b_o)\n",
    "    \n",
    "    return [activacion_z, activacion_a, activacion_h, activacion_o]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio: Realiza un ejemplo pequeño a mano, imprime las activaciones y compruebalo con tus resultados obtenidos manualmente\n",
    "\n",
    "Se agrega un ejemplo sin calcular manualmente. Posiblemente sea mejor establecer W y b en forma manual que faciliten los calculos y menos ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codificación 'one hot': \n",
      " [[0 1 3 9 0 1]\n",
      " [0 4 5 6 0 4]]\n",
      "Autocodificador: \n",
      " [[ 1.76405235 -0.97727788  0.33367433 -0.4380743   1.76405235 -0.97727788]\n",
      " [ 0.40015721  0.95008842  1.49407907 -1.25279536  0.40015721  0.95008842]\n",
      " [ 0.97873798 -0.15135721 -0.20515826  0.77749036  0.97873798 -0.15135721]\n",
      " [ 2.2408932  -0.10321885  0.3130677  -1.61389785  2.2408932  -0.10321885]\n",
      " [ 1.86755799  0.4105985  -0.85409574 -0.21274028  1.86755799  0.4105985 ]\n",
      " [ 1.76405235 -2.55298982 -1.45436567  0.15494743  1.76405235 -2.55298982]\n",
      " [ 0.40015721  0.6536186   0.04575852  0.37816252  0.40015721  0.6536186 ]\n",
      " [ 0.97873798  0.8644362  -0.18718385 -0.88778575  0.97873798  0.8644362 ]\n",
      " [ 2.2408932  -0.74216502  1.53277921 -1.98079647  2.2408932  -0.74216502]\n",
      " [ 1.86755799  2.26975462  1.46935877 -0.34791215  1.86755799  2.26975462]]\n",
      "Activacion capa oculta:\n",
      " [[ 0.          1.30377837  0.          4.2301833   0.          1.30377837]\n",
      " [ 0.          0.          0.          5.51935272  0.          0.        ]\n",
      " [ 0.          7.83296367  0.          3.06781597  0.          7.83296367]\n",
      " [ 6.72215668  2.60667548  4.49524539  0.          6.72215668  2.60667548]\n",
      " [ 6.13088619  0.          0.          0.75006167  6.13088619  0.        ]\n",
      " [15.99118816  0.          0.          0.         15.99118816  0.        ]\n",
      " [ 7.32841956  0.          1.87432705  0.          7.32841956  0.        ]]\n",
      "Salidas:\n",
      " [[0.28211383 0.99999993 0.98835902 0.96619412 0.28211383 0.99999993]]\n"
     ]
    }
   ],
   "source": [
    "vocab = ['a', 'e', 'ei', 'ti', 'tu', 'ya', 'ye', 'toto', 'tur', 'er', 'OOV']\n",
    "\n",
    "X = np.array([\n",
    "    ['a', 'a'],\n",
    "    ['e', 'tu'],\n",
    "    ['ti', 'ya'],\n",
    "    ['er', 'ye'],\n",
    "    ['a', 'a'],\n",
    "    ['e', 'tu']\n",
    "])\n",
    "\n",
    "n_v, n_a, n_h = len(vocab), 5, 7\n",
    "W, b = inicializa_red(n_v, n_a, n_h)\n",
    "A = feedforward(X, vocab, W, b)\n",
    "\n",
    "print(\"Codificación 'one hot': \\n\", A[0])\n",
    "print(\"Autocodificador: \\n\", A[1])\n",
    "print(\"Activacion capa oculta:\\n\", A[2])\n",
    "print(\"Salidas:\\n\", A[3])\n",
    "\n",
    "assert np.all(A[0][:, 1] == A[0][:, -1]) and np.all(A[0][:, 0] == A[0][:, -2])\n",
    "assert np.all(A[1][:, 1] == A[1][:, -1]) and np.all(A[1][:, 0] == A[1][:, -2])\n",
    "assert np.all(A[2][:, 1] == A[2][:, -1]) and np.all(A[2][:, 0] == A[2][:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_relu(a):\n",
    "    \"\"\"\n",
    "    Calcula la derivada de la activación de a usando ReLU\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.where(a > 0.0, 1.0, 0.0)\n",
    "\n",
    "def b_prop(A, Y, W):\n",
    "    \n",
    "    W_a, W_h, W_o = W\n",
    "    activacion_z, activacion_a, activacion_h, activacion_o = A\n",
    "    \n",
    "    delta_o = Y.reshape(1, -1) - activacion_o\n",
    "    delta_h = deriv_relu(activacion_h) * (W_o.T @ delta_o)\n",
    "    delta_a = W_h.T @ delta_h\n",
    "    \n",
    "    gradiente_W_o = delta_o.T @ activacion_h\n",
    "    gradiente_W_h = delta_h.T @ activacion_a\n",
    "    \n",
    "    gradiente_b_o = delta_o.mean(axis=0).reshape(-1, 1)\n",
    "    gradiente_b_h = delta_h.mean()\n",
    "    \n",
    "    gradiente_W_a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
